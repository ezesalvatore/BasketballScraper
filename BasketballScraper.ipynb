{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jNe3aaT3xEtIfjQlYpSGAtu_WW-94GVK",
      "authorship_tag": "ABX9TyOSaTUqffOj7bx5t6fq8kbO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezesalvatore/BasketballScraper/blob/main/BasketballScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BasketBall Reference Uniform Scraper"
      ],
      "metadata": {
        "id": "x74FANU9a1s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project overview\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrkFzG-ptG9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "- Extract players name, team and uniform number\n",
        "- Support wireframe development\n",
        "- Shows my webscraping expertises\n",
        "\n",
        "### Methods used\n",
        "- The webscraper being used is Beautiful Soup, since Basketball Reference is primarily build with HTML\n",
        "- This project also make sure that it follows Basketball Reference rate limiting and crawl-delay violations"
      ],
      "metadata": {
        "id": "PFbJwK4Xy8Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WebScraping Workflow\n",
        "\n",
        "Session `->` Basketball Reference HTML `->` Beautiful Soup Parser `->` Uniform Data `->` Stats CSV Intergration `->` Final CSV"
      ],
      "metadata": {
        "id": "hPZuut4vivFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "snKzXEmfyrK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Makes a request to Basketball Reference\n",
        "import requests\n",
        "\n",
        "#How we are going to webscrape, able to parse through HTML to get the name, team, and uniform\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#Helps me with exporting a csv file\n",
        "import pandas as pd\n",
        "\n",
        "#Allows me to add delays\n",
        "import time\n",
        "\n",
        "#Data processing and cleaning\n",
        "import re\n",
        "\n",
        "#Creates url for web scraping\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Handle Unicode normalization for special characters/accents\n",
        "import unicodedata\n",
        "\n",
        "#Get rid of errors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported\")"
      ],
      "metadata": {
        "id": "ykeBZbWRy3SI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b910392-09e7-47fb-82a9-d010fd975002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Session Setup"
      ],
      "metadata": {
        "id": "G7D5IX6C42yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_session():\n",
        "    \"\"\"\n",
        "    This function is setting up the sessions for web scraping. Making sure the headers are compliant.\n",
        "\n",
        "    \"\"\"\n",
        "    session = requests.Session()\n",
        "\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml',\n",
        "        'Connection': 'keep-alive',\n",
        "    })\n",
        "\n",
        "    print(\"Session is created!\")\n",
        "\n",
        "    return session"
      ],
      "metadata": {
        "id": "HQSVR8cP46jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching Basketball Reference Data"
      ],
      "metadata": {
        "id": "Y7qOsl9f7f8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_basketball_data():\n",
        "    \"\"\"\n",
        "    Make single compliant request to Basketball Reference with proper encoding.\n",
        "    \"\"\"\n",
        "    url = \"https://www.basketball-reference.com/leagues/NBA_2025_numbers.html\"\n",
        "    session = setup_session()\n",
        "\n",
        "    try:\n",
        "        # Robots.txt compliance: Crawl-delay: 3\n",
        "        time.sleep(3.0)\n",
        "\n",
        "        response = session.get(url, timeout=30)\n",
        "\n",
        "        if response.status_code == 429:\n",
        "            raise Exception(\"Rate limited - blocked for 24 hours\")\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Fix encoding if needed\n",
        "        if response.encoding != 'utf-8':\n",
        "            response.encoding = 'utf-8'\n",
        "\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        session.close()"
      ],
      "metadata": {
        "id": "qrXYEOj270k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping"
      ],
      "metadata": {
        "id": "FV-FwvHQ2sNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraping Strategy\n",
        "\n",
        "I will use the the CSS Selector to get the values from the html site\n",
        "\n",
        "**Uniform Number** : `div.data_grid_box table caption`\n",
        "<br>\n",
        "\n",
        "**Player Name**: `div.data_grid_box a[href*='/players/']`\n",
        "<br>\n",
        "\n",
        "**Team**: `span.desc a[href*='/teams/'] `\n",
        "\n"
      ],
      "metadata": {
        "id": "jTxlsmap26KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_uniform_data(html_content):\n",
        "    \"\"\"\n",
        "    Extract player names, teams, and uniform numbers from Basketball Reference HTML.\n",
        "    \"\"\"\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    player_uniform_data = {}\n",
        "\n",
        "    print(\"Parsing HTML content...\")\n",
        "\n",
        "    # Find all uniform number sections\n",
        "    uniform_sections = soup.find_all('div', {'class': 'data_grid_box'})\n",
        "\n",
        "    for section in uniform_sections:\n",
        "        # Extract uniform number from caption\n",
        "        caption = section.find('caption')\n",
        "        if not caption:\n",
        "            continue\n",
        "\n",
        "        uniform_number = caption.get_text().strip()\n",
        "\n",
        "        print(f\"Processing uniform #{uniform_number}...\")\n",
        "\n",
        "        # Find all player rows in this uniform section\n",
        "        player_rows = section.find_all('tr')\n",
        "\n",
        "        for row in player_rows:\n",
        "            # Extract player name\n",
        "            player_link = row.find('a', href=lambda x: x and '/players/' in x)\n",
        "            if not player_link:\n",
        "                continue\n",
        "\n",
        "            player_name = player_link.get_text().strip()\n",
        "\n",
        "            # Extract team(s) - could be multiple teams separated by dashes\n",
        "            team_spans = row.find_all('span', {'class': 'desc'})\n",
        "            teams = []\n",
        "\n",
        "            for span in team_spans:\n",
        "                team_links = span.find_all('a', href=lambda x: x and '/teams/' in x)\n",
        "                for team_link in team_links:\n",
        "                    team_code = team_link.get_text().strip()\n",
        "                    teams.append(team_code)\n",
        "\n",
        "            # Store player data\n",
        "            if player_name and teams:\n",
        "                if player_name not in player_uniform_data:\n",
        "                    player_uniform_data[player_name] = []\n",
        "\n",
        "                for team in teams:\n",
        "                    player_uniform_data[player_name].append((uniform_number, team))\n",
        "\n",
        "    print(f\"✅ Extracted data for {len(player_uniform_data)} players\")\n",
        "    return player_uniform_data"
      ],
      "metadata": {
        "id": "Zz0IZZF2Ch4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating final_csv"
      ],
      "metadata": {
        "id": "4E-m7Pc7n7ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_name(name):\n",
        "    \"\"\"\n",
        "    Remove accents and special characters from player names for matching.\n",
        "    Handles NaN values and edge cases safely.\n",
        "    \"\"\"\n",
        "    # Handle NaN, None, or empty values\n",
        "    if pd.isna(name) or not name or str(name).strip() == '':\n",
        "        return None\n",
        "\n",
        "    # Convert to string and strip whitespace\n",
        "    name_str = str(name).strip()\n",
        "    if not name_str:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Normalize to NFD (decomposed form) then remove combining characters\n",
        "        normalized = unicodedata.normalize('NFD', name_str)\n",
        "        ascii_name = ''.join(c for c in normalized if not unicodedata.combining(c))\n",
        "        return ascii_name\n",
        "    except Exception:\n",
        "        return name_str  # Return original if normalization fails"
      ],
      "metadata": {
        "id": "2KHasfSfp59h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_csv(player_uniform_data, stats_csv_path):\n",
        "    \"\"\"\n",
        "    Combine uniform data with existing stats CSV.\n",
        "    Uses name normalization to handle special characters/accents.\n",
        "    \"\"\"\n",
        "    # Load existing stats\n",
        "    try:\n",
        "        stats_df = pd.read_csv(stats_csv_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Could not find {stats_csv_path}\")\n",
        "        return None\n",
        "\n",
        "    # Data cleaning: Remove rows with missing player names\n",
        "    stats_df = stats_df.dropna(subset=['Player'])\n",
        "    stats_df = stats_df[stats_df['Player'].str.strip() != '']\n",
        "\n",
        "    # Add new columns\n",
        "    stats_df['Uniform'] = ''\n",
        "    stats_df['mult_team'] = False\n",
        "\n",
        "    # Create normalized lookup dictionary for uniform data\n",
        "    normalized_uniform_data = {}\n",
        "    for player_name, uniform_list in player_uniform_data.items():\n",
        "        normalized_key = normalize_name(player_name)\n",
        "        if normalized_key:\n",
        "            normalized_uniform_data[normalized_key] = {\n",
        "                'original_name': player_name,\n",
        "                'uniforms': uniform_list\n",
        "            }\n",
        "\n",
        "    # Process each player in stats\n",
        "    for idx, row in stats_df.iterrows():\n",
        "        player_name = row['Player']\n",
        "        team_code = row['Team']\n",
        "\n",
        "        # Normalize the player name from stats CSV\n",
        "        normalized_stats_name = normalize_name(player_name)\n",
        "\n",
        "        if normalized_stats_name and normalized_stats_name in normalized_uniform_data:\n",
        "            uniform_info = normalized_uniform_data[normalized_stats_name]\n",
        "            uniform_assignments = uniform_info['uniforms']\n",
        "\n",
        "            # Check if player has multiple teams\n",
        "            player_teams = [team for _, team in uniform_assignments]\n",
        "            is_multi_team = len(set(player_teams)) > 1\n",
        "\n",
        "            if is_multi_team and str(team_code).endswith('TM'):\n",
        "                # This is a season total row - create complex uniform string\n",
        "                uniform_parts = []\n",
        "                for uniform_num, team in uniform_assignments:\n",
        "                    uniform_parts.append(f\"{uniform_num}-{team}\")\n",
        "                stats_df.at[idx, 'Uniform'] = \"|\".join(uniform_parts)\n",
        "                stats_df.at[idx, 'mult_team'] = True\n",
        "\n",
        "            else:\n",
        "                # Find specific uniform for this team\n",
        "                team_uniform = None\n",
        "                for uniform_num, team in uniform_assignments:\n",
        "                    if team == team_code:\n",
        "                        team_uniform = uniform_num\n",
        "                        break\n",
        "\n",
        "                if team_uniform:\n",
        "                    stats_df.at[idx, 'Uniform'] = team_uniform\n",
        "                    stats_df.at[idx, 'mult_team'] = is_multi_team\n",
        "\n",
        "    return stats_df"
      ],
      "metadata": {
        "id": "pmOYbS0boC2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Completed the workflow"
      ],
      "metadata": {
        "id": "CyVBpJufhjh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Execute complete workflow: scrape → extract → combine → export\"\"\"\n",
        "\n",
        "    # Step 1: Scrape Basketball Reference\n",
        "    html_content = fetch_basketball_data()\n",
        "    if not html_content:\n",
        "        print(\"❌ Failed to fetch HTML content\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Extract uniform data\n",
        "    player_number_teams = extract_uniform_data(html_content)\n",
        "    if not player_number_teams:\n",
        "        print(\"❌ No uniform data extracted\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Combine with stats CSV\n",
        "    stats_csv_path = '/content/drive/MyDrive/Basketball-Data/baskeball-stats.csv'\n",
        "    final_df = create_final_csv(player_number_teams, stats_csv_path)\n",
        "    if final_df is None:\n",
        "        return None\n",
        "\n",
        "    # Step 4: Export final CSV\n",
        "    output_filename = '/content/drive/MyDrive/Basketball-Data/nba_2025_final.csv'\n",
        "    final_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"✅ Successfully exported {len(final_df)} records to: {output_filename}\")\n",
        "    return final_df\n",
        "\n",
        "# Execute the workflow\n",
        "if __name__ == \"__main__\":\n",
        "    basketball_data = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uawMBzdXhods",
        "outputId": "af817193-e966-4e95-e86a-1651b5c78cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session is created!\n",
            "Parsing HTML content...\n",
            "Processing uniform #00...\n",
            "Processing uniform #0...\n",
            "Processing uniform #1...\n",
            "Processing uniform #2...\n",
            "Processing uniform #3...\n",
            "Processing uniform #4...\n",
            "Processing uniform #5...\n",
            "Processing uniform #6...\n",
            "Processing uniform #7...\n",
            "Processing uniform #8...\n",
            "Processing uniform #9...\n",
            "Processing uniform #10...\n",
            "Processing uniform #11...\n",
            "Processing uniform #12...\n",
            "Processing uniform #13...\n",
            "Processing uniform #14...\n",
            "Processing uniform #15...\n",
            "Processing uniform #16...\n",
            "Processing uniform #17...\n",
            "Processing uniform #18...\n",
            "Processing uniform #19...\n",
            "Processing uniform #20...\n",
            "Processing uniform #21...\n",
            "Processing uniform #22...\n",
            "Processing uniform #23...\n",
            "Processing uniform #24...\n",
            "Processing uniform #25...\n",
            "Processing uniform #26...\n",
            "Processing uniform #27...\n",
            "Processing uniform #28...\n",
            "Processing uniform #29...\n",
            "Processing uniform #30...\n",
            "Processing uniform #31...\n",
            "Processing uniform #32...\n",
            "Processing uniform #33...\n",
            "Processing uniform #34...\n",
            "Processing uniform #35...\n",
            "Processing uniform #36...\n",
            "Processing uniform #37...\n",
            "Processing uniform #40...\n",
            "Processing uniform #41...\n",
            "Processing uniform #42...\n",
            "Processing uniform #43...\n",
            "Processing uniform #44...\n",
            "Processing uniform #45...\n",
            "Processing uniform #46...\n",
            "Processing uniform #50...\n",
            "Processing uniform #54...\n",
            "Processing uniform #55...\n",
            "Processing uniform #58...\n",
            "Processing uniform #61...\n",
            "Processing uniform #65...\n",
            "Processing uniform #67...\n",
            "Processing uniform #71...\n",
            "Processing uniform #76...\n",
            "Processing uniform #77...\n",
            "Processing uniform #88...\n",
            "Processing uniform #91...\n",
            "Processing uniform #94...\n",
            "Processing uniform #99...\n",
            "✅ Extracted data for 569 players\n",
            "✅ Successfully exported 736 records to: /content/drive/MyDrive/Basketball-Data/nba_2025_final.csv\n"
          ]
        }
      ]
    }
  ]
}